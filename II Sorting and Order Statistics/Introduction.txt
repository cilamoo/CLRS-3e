1.The structure of the data

If each record includes a large amount of satellite data, we often 
permute an array of pointers to the records rather than the records 
themselves in order to minimize data movement.

2.Why sorting?

We can prove a nontrivial lower bound for sorting (as we shall do in Chapter 8).
Our best upper bounds match the lower bound asymptotically, and so we know
that our sorting algorithms are asymptotically optimal. Moreover, we can use
the lower bound for sorting to prove lower bounds for certain other problems.

3.Sorting algorithms
Because insertion sort inner loops are tight, however, it is a fast in-place sorting algorithm for small input sizes.

Merge sort has a better asymptotic running time, Θ(nlgn) , but theMERGE procedure it uses does not operate in place.

a sorting algorithm sorts in place if only a constant number of elements of the input array
are ever stored outside the array.

Quicksort, in Chapter 7, also sorts n numbers in place, but its worst-case running
time is Θ(n^2). Its expected running time is Θ(nlgn), however, and it generally
outperforms heapsort in practice. Like insertion sort, quicksort has tight code, and
so the hidden constant factor in its running time is small. It is a popular algorithm
for sorting large input arrays.

Chapter 8 then goes on to show that we can beat this lower bound of Ω(nlgn)
if we can gather information about the sorted order of the input by means other
than comparing elements.

The counting sort algorithm, for example, assumes that
the input numbers are in the set {0,1,...,k}. By using array indexing as a tool
for determining relative order, counting sort can sort n numbers in Θ(k+n) time.

