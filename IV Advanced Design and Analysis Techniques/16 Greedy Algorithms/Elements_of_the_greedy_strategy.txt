16.2 Elements of the greedy strategy

More generally, we design greedy algorithms according to the following sequence
of steps:
1. Cast the optimization problem as one in which we make a choice and are left
with one subproblem to solve.

2. Prove that there is always an optimal solution to the original problem that makes
the greedy choice, so that the greedy choice is always safe.

3. Demonstrate optimal substructure by showing that, having made the greedy
choice, what remains is a subproblem with the property that if we combine an
optimal solution to the subproblem with the greedy choice we have made, we
arrive at an optimal solution to the original problem.

How can we tell whether a greedy algorithm will solve a particular optimization
problem? No way works all the time, but the greedy-choice property and optimal
substructure are the two key ingredients. If we can demonstrate that the problem
has these properties, then we are well on the way to developing a greedy algorithm
for it.

1.Greedy-choice property
The first key ingredient is the greedy-choice property: we can assemble a globally
optimal solution by making locally optimal (greedy) choices. In other words, when
we are considering which choice to make, we make the choice that looks best in
the current problem, without considering results from subproblems.

By preprocessing the input or by using an appropriate data structure (often a priority queue),
we often can make greedy choices quickly, thus yielding an efficient algorithm.

2.Optimal substructure
A problem exhibits optimal substructure if an optimal solution to the problem
contains within it optimal solutions to subproblems. This property is a key ingredient
of assessing the applicability of dynamic programming as well as greedy algorithms.

All we really need to do is argue that an optimal solution to the subproblem,
combined with the greedy choice already made, yields an optimal solution to the 
original problem. This scheme implicitly uses induction on the subproblems to prove 
that making the greedy choice at every step produces an optimal solution.

3.Greedy versus dynamic programming
we can solve the fractional knapsack problem by a greedy strategy, but we cannot 
solve the 0-1 problem by such a strategy.

In the 0-1 problem, when we consider whether to include an item in the knapsack, 
we must compare the solution to the subproblem that includes the item with the 
solution to the subproblem that excludes the item before we can make the choice.

4.TODO:
The problem formulated in this way gives rise to many overlapping subproblemsâ€”
a hallmark of dynamic programming, and indeed, as Exercise 16.2-2
asks you to show, we can use dynamic programming to solve the 0-1 problem.